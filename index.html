<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering?</title>
  <script type="module" src="https://md-block.verou.me/md-block.js"></script>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://github.com/HKUSTDial">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/HKUSTDial">
            HKUST(GZ) DIAL
          </a>
          <!-- <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape -->
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering?</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Evanwu1125">Yifan Wu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://lutaoyan.github.io/">Yanlu Tao</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://shenleixian.github.io/">Leixian Shen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.yunhaiwang.net/">Yunhai Wang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://nantang.github.io/">Nan Tang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://luoyuyu.vip/">Yuyu Luo</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology (Guangzhou),</span>
            <span class="author-block"><sup>2</sup>The Hong Kong University of Science and Technology</span>
            <span class="author-block"><sup>3</sup>South China University of Technology</span>
            <span class="author-block"><sup>4</sup>Renmin University of China</span>
          </div>

          <!-- æ–°å¢žçš„ä¸¤ä¸ªç”µå­é‚®ä»¶åœ°å€ -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <i class="fas fa-envelope"></i> Yifan Wu: <span style="color: blue;">ywu012@connect.hkust-gz.edu.cn</span>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <i class="fas fa-envelope"></i> Yuyu Luo: <span style="color: blue;">yuyuluo@hkust-gz.edu.cn</span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            </strong><span class="author-block" style="color: rgb(145, 107, 164); font-weight: bold;">ðŸŽ‰EMNLP 2024 (Findings)ðŸŽ‰</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.07001"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.07001"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/HKUSTDial/ChartInsights"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/HKUSTDial/ChartInsights/tree/master/Dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
      <img id="teaser" autoplay muted loop playsinline height="90%" src="static\images\chartinsights_new_pipeline.jpg"/>
      <h2 class="subtitle has-text-centered">
        An overview of Experimental Settings
      </h2>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-wdith">
        <h2 class="title is-2">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Chart question answering (ChartQA) tasks play a critical role in interpreting and extracting insights from visualization charts. While recent advancements 
            in multimodal large language models (MLLMs) like GPT-4o have shown promise in high-level ChartQA tasks, such as chart captioning, their effectiveness in low-level ChartQA tasks (<em>e.g.</em> identifying correlations) remains underexplored.
            In this paper, we address this gap by evaluating MLLMs on low-level ChartQA using a newly curated dataset, <strong>ChartInsights</strong>,
            which consists of 22,347 (chart, task, query, answer) covering 10 data analysis tasks across 7 chart types. 
          </p>
          <p>  
            We systematically evaluate 19 advanced MLLMs, including 12 open-source and 7 closed-source models. The average accuracy rate across these models is 39.8%, with GPT-4o achieving the highest accuracy at 69.17%.
            To further explore the limitations of MLLMs in low-level ChartQA, we conduct experiments that alter visual elements of charts (<em>e.g.</em> changing color schemes, adding image noise) to assess their impact on the task effectiveness. 
            Furthermore, we propose a new textual prompt strategy, <strong>Chain-of-Charts</strong>, tailored for low-level ChartQA tasks,  which boosts performance by 14.41%, achieving an accuracy of 83.58%. 
            Finally, incorporating a visual prompt strategy that directs attention to relevant visual elements further improves accuracy to 84.32%. 
          </p>
          <p>Our contributions are summarized as follows:</p>

          <md-block>
            - <strong>ChartInsights Dataset.</strong>  We curate ChartInsights, <strong>the first</strong> dataset for evaluating 
            low-level data analysis tasks on charts. ChartInsights includes diverse chart variants, textual and visual prompts, 
            and comprehensive metadata, enabling the investigation of MLLMs' performance across various low-level ChartQA scenarios.
            - <strong>Systematic Evaluation.</strong> Our study establishes benchmarks by evaluating 19 MLLMs on 10 low-level 
            ChartQA tasks, providing valuable insights into the current capabilities of MLLMs in processing and analyzing chart information.
            - <strong>Provide meaningful insights.</strong> We conduct a thorough analysis of the experimental results and identify 12 key findings. 
            These insights emphasize the critical role of visual prompts, chart elements, and image quality in successfully performing low-level 
            tasks.
            - <strong>New Prompt Strategy.</strong> We introduce the <strong>Chain-of-Charts</strong> strategy, 
            a new textual prompt designed to enhance MLLMs' reasoning capabilities in ChartQA tasks by leveraging a series of interconnected 
            question-answer pairs to guide the model.
 
          </md-block>
          <!-- <p>&ensp;</p>
          <div class="container is-max-desktop">
            <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/human_statistician_thinking.png"/>
            <h5 class="subtitle has-text-centered">
              An Example of Statistical Analysis Task
            </h5>
          </div> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>



<!-- StatQA construction  -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-2">ChartInsights Construction</h2>
        <p>&ensp;</p>
        <div class="columns is-centered">
          <div class="content has-text-justified">
            <md-block>
              To fulfill our three design goals, our construction process begins with the collection of charts
              with metadata from existing datasets. After
              collecting and reviewing a large number of
              datasets, we decided to extract charts from
              nvBench and ChartQA. The reason is that most
              charts in these two datasets contain numerical
              information of elements, which can meet the
              requirements for 10 low-level ChartQA tasks.
              We extracted approximately 900 charts from
              ChartQA and about 1100 charts from nvBench. Next,
              we meticulously assign specific low-level data analysis tasks to appropriate chart types. Lastly, we
              develop diverse textual prompt strategies, along
              with visual variants and prompts, tailored to each
              chart. Note that we save all metadata during the
              construction process, which can make the users89,388 Questions customize their dataset based on ChartInsights easily.
              
              As shown in the construction pipeline of ChartInsights, the construction of our
              ChartInsights consists of five steps: Candidate
              Charts Selection, Low-Level Tasks Generation,
              Textual Prompts Design, Visual Variants Generation, and Visual Prompts Design.
            </md-block>
            <p>&ensp;</p>
            <div class="container is-max-desktop">
              <img id="teaser" autoplay muted loop playsinline height="90%" src="static\images\chartinsights_construction.jpg"/>
              <h5 class="subtitle has-text-centered">
                Construction Pipeline of ChartInsights
              </h5>
            </div>
            <div class="container is-max-desktop">
              <img id="teaser" autoplay muted loop playsinline height="90%" src="static\images\ChartInsights_stasitics.png"/>
              <h5 class="subtitle has-text-centered">
                The Statistics of ChartInsights
              </h5>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Experiments -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-centered">
      <h2 class="title is-2">Experiments</h2>
      <p>&ensp;</p>
      <div class="columns is-centered">
        <div class="content has-text-justified">
          The experiments we conducted are listed as follows:<br>

          <md-block>
            <strong>Exp1 Benchmarking MLLMs:</strong> <br>
            We start by benchmarking the performance of widely used MLLMs across 10 
            low-level ChartQA tasks involving 7 different types of charts. This experiment establishes a baseline for 
            understanding the capabilities and limitations of MLLMs in low-level ChartQA tasks.
            
            <strong>Exp-2 Impact of Question Types:</strong><br> We analyze how different question types influence MLLMs interactions
            , helping to identify which types elicit the most accurate and informative responses.
            
            <strong>Exp-3 Textual Prompt Strategies:</strong><br> We investigate the effect of various textual prompt strategies, 
            such as Chain-of-Thoughts, on MLLMs performance.

            <strong>Exp-4 Impact of Visual Prompts:</strong><br> We conduct an in-depth exploration of the impact of visual prompts
             on MLLMs performance to understand how guiding the MLLMs' attention to specific visual elements can enhance its 
             analytical capabilities.

             <strong>Exp-5 Impact of Chart Variantions:</strong><br> We vary chart elements to analyze how changes in color schemes
             , view sizes, and legends affect the  performance.

             <strong>Exp-6 Impact of Image Quality:</strong><br> We evaluate the effect of image quality by introducing various 
             levels of image perturbations, such as noise and resolution changes, to understand the robustness of MLLMs 
             in handling low-quality charts.

             <strong>Exp-7 Synergistic Effects of Different Strategies:</strong><br> Finally, we explore the synergistic effects
              of combining different question types, textual prompts, and visual prompts to enhance the overall performance
               of MLLMs in low-level ChartQA tasks.

          </md-block>
          <p>&ensp;</p>
          <div class="container is-max-desktop">
            <img id="teaser" autoplay muted loop playsinline height="90%" src="static\images\CharInsights_results1.jpg"/>
            <h5 class="subtitle has-text-centered">
              Heatmap Results of Experiment 1 - 7
            </h5>
          </div>
          <div class="container is-max-desktop">
            <img id="teaser" autoplay muted loop playsinline height="90%"src="static\images\ChartInsights_results2.jpg"/>
            <h5 class="subtitle has-text-centered">
              Radar Chart Results of Experiment 1 - 7
            </h5>
          </div>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>


<!-- Findins -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-centered">
      <h2 class="title is-2">Findings and Take-aways</h2>
      <p>&ensp;</p>
      <div class="columns is-centered">
        <div class="content has-text-justified">
          <!-- <p>We summarize six key findings through our systematic experiments and analysis:</p> -->
          <md-block>
            We summarize 12 key findings through our systematic experiments and analysis:
            - <strong>Findings-1:</strong> Closed-Source models exhibit far superior generalization performance in low-level analysis tasks compared to open-source models.
            - <strong>Findings-2:</strong> The ability of open-source models to understand low-level charts is not directly proportional to their number of model parameters.
            - <strong>Findings-3:</strong> We find open-source models like ViP-LLaVA show higher accuracy on Yes-or-No questions because of a possible bias towards 'No' labels.
            - <strong>Findings-4:</strong> The performance of GPT-4o declines as task complexity increases, mirroring human performance, but it has not yet matched the analytical capabilities of average humans.
            - <strong>Findings-5:</strong> Structured textual prompts and candidate answers significantly enhance GPT-4o's ability to reason out correct responses.
            - <strong>Findings-6:</strong> Chain-of-Charts supplies GPT-4o with accurate chart reference information, enhancing the model's comprehension and detailed reasoning of chart structures and elements.
            - <strong>Findings-7:</strong> Visual prompts greatly improve GPT-4o's performance, showing the value of visual information in aiding comprehension and reasoning.
            - <strong>Findings-8:</strong> Different tasks require tailored visual prompts for effective chart comprehension. Using the same style of visual prompts across tasks can have a negative impact on certain tasks.
            - <strong>Findings-9:</strong> While most chart variants have a minimal impact on GPT-4o's performance, the absence of data labels significantly affects its accuracy. Additionally, larger labels and the removal of data labels can actually enhance GPT-4o's performance in anomaly detection and filtering tasks, as it shifts its focus to visual comparisons.
            - <strong>Findings-10:</strong> GPT-4o's accuracy varies under the influence of different types of noise on various charts. Interestingly, there are instances where the accuracy improves, particularly in visually semantic tasks. This suggests that GPT-4o can rely more on visual information when the textual information is compromised.
            - <strong>Findings-11:</strong> Combining visual prompts with the
            Chain-of-Charts strategy significantly improves
            the performance, suggesting that integrating multiple types of prompts can leverage their respective strengths.
            - <strong>Findings-12:</strong> Adding a Visual Prompt improves performance, but its impact is limited when applied to the Chain-of-Charts strategy.
          </md-block>
          <p>&ensp;</p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>





<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2406.07815">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/HKUSTDial" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
